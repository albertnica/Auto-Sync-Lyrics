{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4f47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncoment if needed\n",
    "\n",
    "# ! pip3 install -r requirements.txt\n",
    "# ! pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu129"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650711d",
   "metadata": {},
   "source": [
    "Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff1e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# IMPORTS\n",
    "# -------------------------\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "import difflib\n",
    "from collections import deque\n",
    "import whisper\n",
    "from mutagen.flac import FLAC\n",
    "from lyricsgenius import Genius\n",
    "\n",
    "# -------------------------\n",
    "# INIT & CONFIG PARAMETERS\n",
    "# -------------------------\n",
    "# Toggle debug verbosity. When DEBUG is True many extra detalles se imprimen a stdout.\n",
    "DEBUG = True\n",
    "\n",
    "# Directory names\n",
    "SONGS_FOLDER = \"songs\"           # where your audio files (.flac) live\n",
    "LYRICS_FOLDER = \"lyrics\"         # output folder where generated .lrc files are written\n",
    "LYRICS_DB_FOLDER = \".lyrics_db\"  # persistent database of processed lyrics (normalized: \"<Title> - <Artist>.lrc\")\n",
    "LOGS_FOLDER = \".logs\"            # folder to collect all raw transcripts, timestamps, raw lyrics and human logs\n",
    "\n",
    "# Credentials\n",
    "CREDENTIALS_PATH = \"credentials.json\"\n",
    "\n",
    "# Output files\n",
    "NO_ANCHORS_FILE = os.path.join(LOGS_FOLDER, \"no_anchors.txt\")  # will list files with no anchors\n",
    "\n",
    "# Whisper model names to compare. Transcriptions will be produced for each model and compared.\n",
    "TRANSCRIBE_MODELS = [\"large-v3\", \"large-v3-turbo\"]\n",
    "\n",
    "# How much extra proportion of time the first non-anchored line receives in head interpolation.\n",
    "# If there are M lines before the first anchored line, weights = [FIRST_LINE_WEIGHT, 1, 1, ..., 1] (length M)\n",
    "FIRST_LINE_WEIGHT = 3\n",
    "\n",
    "# Silence / thresholds\n",
    "MIN_SILENCE_DURATION = 1.5        # minimal gap between word end and next word start to consider a \"silence\"\n",
    "LONG_SILENCE_THRESHOLD = 10.0     # silence >= this is considered a \"long silence\" and blocks interpolation across it\n",
    "THRESH_ANCHOR = 0.80              # minimal similarity score for an anchor candidate\n",
    "MIN_OVERLAP = 0.60                # minimal fraction of lyric words present in matched transcription window\n",
    "MIN_ANCHOR_SPACING = 2.0          # minimum seconds between accepted anchors (to avoid clustering anchors too close)\n",
    "\n",
    "# Timestamp progression / fallback\n",
    "MIN_LINE_PROGRESSION = 0.25       # minimal increment to enforce strictly increasing timestamps\n",
    "FALLBACK_SPACING = 2.5            # spacing used when filling the tail region without active intervals\n",
    "\n",
    "# -------------------------\n",
    "# LOGGING HELPERS\n",
    "# -------------------------\n",
    "def dbg(msg: str):\n",
    "    \"\"\"Debug printing: prints only when DEBUG is True.\"\"\"\n",
    "    if DEBUG:\n",
    "        print(\"[DEBUG]\", msg)\n",
    "\n",
    "def info(msg: str):\n",
    "    \"\"\"\n",
    "    Informational printing: prints only essential runtime status.\n",
    "    The idea is to keep the console clean when DEBUG is False.\n",
    "    \"\"\"\n",
    "    print(\"[INFO]\", msg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a953e2e6",
   "metadata": {},
   "source": [
    "Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed028a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# DATA EXTRACTION\n",
    "# -------------------------\n",
    "def ensure_folders():\n",
    "    \"\"\"Ensure all expected folders exist.\"\"\"\n",
    "    os.makedirs(SONGS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(LYRICS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(LYRICS_DB_FOLDER, exist_ok=True)\n",
    "    os.makedirs(LOGS_FOLDER, exist_ok=True)\n",
    "\n",
    "def load_credentials(path: str = CREDENTIALS_PATH) -> dict:\n",
    "    \"\"\"Load optional credentials.json (e.g. Genius token). Returns dict or empty dict.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        dbg(f\"credentials not found at {path}\")\n",
    "        return {}\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, dict):\n",
    "                dbg(\"credentials loaded\")\n",
    "                return data\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error loading credentials: {e}\")\n",
    "    return {}\n",
    "\n",
    "def find_flac_files(folder: str = SONGS_FOLDER) -> List[str]:\n",
    "    \"\"\"Return sorted list of .flac files in songs folder.\"\"\"\n",
    "    return sorted(glob.glob(os.path.join(folder, \"*.flac\")))\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"Lowercase and remove punctuation (preserve apostrophes).\"\"\"\n",
    "    s = (s or \"\").lower()\n",
    "    s = re.sub(r\"[^\\w\\s']\", \" \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def sanitize_filename(text: str) -> str:\n",
    "    \"\"\"Return a file-system-friendly filename from text.\"\"\"\n",
    "    text = re.sub(r'[<>:\"/\\\\|?*]', '', text)\n",
    "    text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) > 200:\n",
    "        text = text[:200].strip()\n",
    "    return text\n",
    "\n",
    "def split_lyrics_lines(lyrics: str) -> List[str]:\n",
    "    \"\"\"Return non-empty lyric lines, skipping bracketed metadata lines like [Chorus].\"\"\"\n",
    "    out = []\n",
    "    for line in (lyrics or \"\").splitlines():\n",
    "        l = line.strip()\n",
    "        if not l:\n",
    "            continue\n",
    "        if re.match(r'^\\[.*\\]$', l):\n",
    "            continue\n",
    "        out.append(l)\n",
    "    return out\n",
    "\n",
    "def extract_metadata_from_flac(path: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Try to extract (artist, title) from FLAC tags using mutagen.\n",
    "    If tags are missing, fall back to filename parsing \"Artist - Title.flac\".\n",
    "    \"\"\"\n",
    "    if os.path.exists(path):\n",
    "        try:\n",
    "            audio = FLAC(path)\n",
    "            artist = audio.get(\"artist\", [None])[0]\n",
    "            title = audio.get(\"title\", [None])[0]\n",
    "            if artist and title:\n",
    "                dbg(f\"metadata from FLAC: artist='{artist}', title='{title}'\")\n",
    "                return artist, title\n",
    "        except Exception:\n",
    "            dbg(\"mutagen could not read FLAC tags (or mutagen not available)\")\n",
    "    base = os.path.splitext(os.path.basename(path))[0]\n",
    "    if \" - \" in base:\n",
    "        parts = base.split(\" - \", 1)\n",
    "        return parts[0].strip(), parts[1].strip()\n",
    "    return None, base\n",
    "\n",
    "def parse_lrc_header_tags(lrc_path: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    \"\"\"\n",
    "    Extract [ti:Title] and [ar:Artist] tags from an existing LRC file (if present).\n",
    "    Only inspects the first ~50 lines to be efficient.\n",
    "    \"\"\"\n",
    "    artist = None\n",
    "    title = None\n",
    "    try:\n",
    "        with open(lrc_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            for _ in range(50):\n",
    "                line = f.readline()\n",
    "                if not line:\n",
    "                    break\n",
    "                line = line.strip()\n",
    "                m_ti = re.match(r'^\\s*\\[ti\\s*:\\s*(.+?)\\s*\\]\\s*$', line, re.I)\n",
    "                m_ar = re.match(r'^\\s*\\[ar\\s*:\\s*(.+?)\\s*\\]\\s*$', line, re.I)\n",
    "                if m_ti:\n",
    "                    title = m_ti.group(1).strip()\n",
    "                if m_ar:\n",
    "                    artist = m_ar.group(1).strip()\n",
    "                if artist and title:\n",
    "                    break\n",
    "    except Exception:\n",
    "        dbg(\"failed to parse LRC header tags\")\n",
    "    return artist, title\n",
    "\n",
    "# -------------------------\n",
    "# LYRICS DB MANAGEMENT\n",
    "# -------------------------\n",
    "def initialize_folders():\n",
    "    \"\"\"Prepare output and DB folders. If lyrics/ exists, recreate clean output folder.\"\"\"\n",
    "    info(\"Initializing folders...\")\n",
    "    if os.path.exists(LYRICS_FOLDER):\n",
    "        info(f\"  Removing existing '{LYRICS_FOLDER}' directory...\")\n",
    "        shutil.rmtree(LYRICS_FOLDER)\n",
    "    os.makedirs(LYRICS_FOLDER, exist_ok=True)\n",
    "    os.makedirs(LYRICS_DB_FOLDER, exist_ok=True)\n",
    "    os.makedirs(LOGS_FOLDER, exist_ok=True)\n",
    "    dbg(f\"created folders: {LYRICS_FOLDER}, {LYRICS_DB_FOLDER}, {LOGS_FOLDER}\")\n",
    "\n",
    "def copy_to_lyrics_db(lrc_path: str, title: str, artist: str):\n",
    "    \"\"\"\n",
    "    Copy a generated LRC into the normalized lyrics DB as \"<Title> - <Artist>.lrc\".\n",
    "    Overwrite existing DB entries to preserve the authoritative LRC (user requested).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        normalized_name = sanitize_filename(f\"{title} - {artist}\")\n",
    "        db_filename = f\"{normalized_name}.lrc\"\n",
    "        db_path = os.path.join(LYRICS_DB_FOLDER, db_filename)\n",
    "        # Overwrite if exists (user requested that songs/*.lrc are authoritative)\n",
    "        try:\n",
    "            if os.path.exists(db_path):\n",
    "                os.remove(db_path)\n",
    "                dbg(f\"removed existing DB entry to overwrite: {db_filename}\")\n",
    "            shutil.copy2(lrc_path, db_path)\n",
    "            dbg(f\"copied to DB: {db_filename}\")\n",
    "        except Exception as e:\n",
    "            info(f\"warning: error copying to DB: {e}\")\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error copying to DB: {e}\")\n",
    "\n",
    "def restore_from_lyrics_db(title: str, artist: str, target_lrc_path: str) -> bool:\n",
    "    \"\"\"If DB contains a matching '<Title> - <Artist>.lrc', copy it to target and return True.\"\"\"\n",
    "    try:\n",
    "        normalized_name = sanitize_filename(f\"{title} - {artist}\")\n",
    "        db_filename = f\"{normalized_name}.lrc\"\n",
    "        db_path = os.path.join(LYRICS_DB_FOLDER, db_filename)\n",
    "        if os.path.exists(db_path):\n",
    "            shutil.copy2(db_path, target_lrc_path)\n",
    "            info(f\"Restored from DB: {db_filename}\")\n",
    "            return True\n",
    "        else:\n",
    "            dbg(f\"not found in DB: {db_filename}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error restoring from DB: {e}\")\n",
    "        return False\n",
    "\n",
    "def search_similar_in_db(title: str, artist: str, similarity_threshold: float = 0.8) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Search for a similar DB entry to avoid fetching/processing when close match exists.\n",
    "    This is useful when metadata slightly differs but we already have a processed LRC.\n",
    "    Returns path to DB file or None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(LYRICS_DB_FOLDER):\n",
    "            return None\n",
    "        db_files = glob.glob(os.path.join(LYRICS_DB_FOLDER, \"*.lrc\"))\n",
    "        if not db_files:\n",
    "            return None\n",
    "        target_string = f\"{title} - {artist}\".lower()\n",
    "        best_match = None\n",
    "        best_score = 0.0\n",
    "        for db_file in db_files:\n",
    "            db_basename = os.path.splitext(os.path.basename(db_file))[0]\n",
    "            db_string = db_basename.lower()\n",
    "            similarity = compute_similarity(target_string, db_string)\n",
    "            if similarity > best_score and similarity >= similarity_threshold:\n",
    "                best_score = similarity\n",
    "                best_match = db_file\n",
    "        if best_match:\n",
    "            info(f\"  Similar found in DB: {os.path.basename(best_match)} (score {best_score:.3f})\")\n",
    "            return best_match\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error searching DB: {e}\")\n",
    "        return None\n",
    "\n",
    "# -------------------------\n",
    "# INGEST EXISTING LRCs & SONGS CLEANUP\n",
    "# -------------------------\n",
    "def ingest_existing_lrcs_and_cleanup_songs():\n",
    "    \"\"\"\n",
    "    Copy any .lrc files that live alongside songs/ into lyrics/ and normalized DB (.lyrics_db).\n",
    "    If an LRC exists in songs/, treat it as authoritative: MOVE it into lyrics/ (replacing\n",
    "    any existing file with the same name in lyrics/) and COPY it into .lyrics_db/ under the\n",
    "    canonical \"<Title> - <Artist>.lrc\" name, OVERWRITING any existing DB entry.\n",
    "    Then remove any remaining non-audio files from the songs/ folder (keep .flac .mp3 .wav).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(SONGS_FOLDER):\n",
    "        dbg(\"songs folder not present; skipping ingest\")\n",
    "        return\n",
    "\n",
    "    info(\"Ingesting .lrc files found under songs/ and cleaning songs/...\")\n",
    "    lrc_patterns = [os.path.join(SONGS_FOLDER, \"*.lrc\"), os.path.join(SONGS_FOLDER, \"*.LRC\")]\n",
    "    lrc_files = []\n",
    "    for p in lrc_patterns:\n",
    "        lrc_files.extend(glob.glob(p))\n",
    "\n",
    "    for lrc_path in lrc_files:\n",
    "        try:\n",
    "            basename = os.path.basename(lrc_path)\n",
    "            dest_lyrics = os.path.join(LYRICS_FOLDER, basename)\n",
    "\n",
    "            # If a file with same name already exists in lyrics/, remove it to ensure authoritative replace.\n",
    "            try:\n",
    "                if os.path.exists(dest_lyrics):\n",
    "                    os.remove(dest_lyrics)\n",
    "                    dbg(f\"existing lyrics file removed to be replaced: {dest_lyrics}\")\n",
    "            except Exception as e:\n",
    "                dbg(f\"could not remove existing lyrics file {dest_lyrics}: {e}\")\n",
    "\n",
    "            # MOVE the LRC from songs/ to lyrics/ (user requested that LRCs in songs are fully correct).\n",
    "            try:\n",
    "                shutil.move(lrc_path, dest_lyrics)\n",
    "                dbg(f\"moved {basename} to {LYRICS_FOLDER}\")\n",
    "            except Exception as e:\n",
    "                # If move fails (permissions, cross-device), fallback to copy+remove\n",
    "                try:\n",
    "                    shutil.copy2(lrc_path, dest_lyrics)\n",
    "                    os.remove(lrc_path)\n",
    "                    dbg(f\"copied then removed original (move fallback) {basename} to {LYRICS_FOLDER}\")\n",
    "                except Exception as e2:\n",
    "                    info(f\"warning: could not move or copy {basename} to {LYRICS_FOLDER}: {e2}\")\n",
    "                    continue  # skip further processing for this file\n",
    "\n",
    "            # choose canonical DB name:\n",
    "            name_no_ext = os.path.splitext(basename)[0]\n",
    "            corresponding_flac = os.path.join(SONGS_FOLDER, f\"{name_no_ext}.flac\")\n",
    "            artist_meta = None\n",
    "            title_meta = None\n",
    "            if os.path.exists(corresponding_flac):\n",
    "                artist_meta, title_meta = extract_metadata_from_flac(corresponding_flac)\n",
    "\n",
    "            # fallback: try to read [ti:] [ar:] tags in the LRC (now in dest_lyrics)\n",
    "            if not (artist_meta and title_meta):\n",
    "                lrc_artist, lrc_title = parse_lrc_header_tags(dest_lyrics)\n",
    "                if lrc_artist and lrc_title:\n",
    "                    if not artist_meta:\n",
    "                        artist_meta = lrc_artist\n",
    "                    if not title_meta:\n",
    "                        title_meta = lrc_title\n",
    "\n",
    "            if artist_meta and title_meta:\n",
    "                canonical_db_name = sanitize_filename(f\"{title_meta} - {artist_meta}\")\n",
    "            else:\n",
    "                canonical_db_name = sanitize_filename(name_no_ext)\n",
    "\n",
    "            db_dest = os.path.join(LYRICS_DB_FOLDER, f\"{canonical_db_name}.lrc\")\n",
    "\n",
    "            # COPY into DB, overwriting any existing entry (authoritative move).\n",
    "            try:\n",
    "                if os.path.exists(db_dest):\n",
    "                    os.remove(db_dest)\n",
    "                    dbg(f\"existing DB entry removed to be replaced: {db_dest}\")\n",
    "                shutil.copy2(dest_lyrics, db_dest)\n",
    "                dbg(f\"copied to DB {os.path.basename(db_dest)} (overwrote if existed)\")\n",
    "            except Exception as e:\n",
    "                info(f\"warning: error copying {dest_lyrics} to DB: {e}\")\n",
    "        except Exception as e:\n",
    "            info(f\"warning: error processing {lrc_path}: {e}\")\n",
    "\n",
    "    # clean non-audio files in songs/\n",
    "    info(\"  Cleaning songs/: deleting non-audio files...\")\n",
    "    allowed_exts = {'.flac', '.mp3', '.wav'}\n",
    "    for entry in os.listdir(SONGS_FOLDER):\n",
    "        fpath = os.path.join(SONGS_FOLDER, entry)\n",
    "        if os.path.isfile(fpath):\n",
    "            ext = os.path.splitext(entry)[1].lower()\n",
    "            if ext not in allowed_exts:\n",
    "                try:\n",
    "                    os.remove(fpath)\n",
    "                    dbg(f\"removed file {entry} from songs/\")\n",
    "                except Exception as e:\n",
    "                    info(f\"warning: could not remove {entry}: {e}\")\n",
    "    dbg(\"ingest and cleanup finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f4cc6e",
   "metadata": {},
   "source": [
    "Plain lyrics retrieving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d29576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# GENIUS HELPERS\n",
    "# -------------------------\n",
    "def get_genius_client(token: str) -> Optional[Genius]:\n",
    "    \"\"\"Return a configured lyricsgenius Genius client or None on failure.\"\"\"\n",
    "    try:\n",
    "        g = Genius(token, timeout=30, retries=3, remove_section_headers=True)\n",
    "        g.verbose = False\n",
    "        g.skip_non_songs = True\n",
    "        g.excluded_terms = [\"(Remix)\", \"(Live)\", \"(Acoustic)\", \"(Instrumental)\"]\n",
    "        dbg(\"Genius client initialized\")\n",
    "        return g\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error initializing Genius: {e}\")\n",
    "        return None\n",
    "\n",
    "def is_translation_content(text: str) -> bool:\n",
    "    \"\"\"Heuristic: return True if text likely refers to a translation or alternate-language lyrics.\"\"\"\n",
    "    text_lower = (text or \"\").lower()\n",
    "    keywords = ['traducción', 'translation', 'traduccion', 'versión', 'versao', 'traduit', 'tradução', 'spanish', 'español']\n",
    "    for kw in keywords:\n",
    "        if kw in text_lower:\n",
    "            return True\n",
    "    patterns = [r'\\b(sub|lyrics?)\\s+(es|español|spanish|pt|português|portuguese)\\b', r'\\[(es|en|pt|fr|de|it)\\]']\n",
    "    for p in patterns:\n",
    "        if re.search(p, text_lower):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_genius_lyrics_simple(artist: str, title: str) -> str:\n",
    "    \"\"\"\n",
    "    Search Genius using a ranked candidate approach and return lyrics text.\n",
    "    Throw RuntimeError on failure so callers can fallback.\n",
    "    \"\"\"\n",
    "    if not artist or not title:\n",
    "        raise RuntimeError(\"Artist/title missing\")\n",
    "    try:\n",
    "        info(f\"  Searching Genius for '{title}' - '{artist}' (simple)...\")\n",
    "        search_query = f\"{title} {artist}\"\n",
    "        results = genius.search_songs(search_query, per_page=20)\n",
    "        if not results or not results.get('hits'):\n",
    "            raise RuntimeError(\"No results\")\n",
    "        candidates = []\n",
    "        for hit in results.get('hits', []):\n",
    "            song_info = hit.get('result', {}) if isinstance(hit, dict) else {}\n",
    "            song_title = song_info.get('title', '')\n",
    "            song_artist = song_info.get('primary_artist', {}).get('name', '')\n",
    "            song_id = song_info.get('id')\n",
    "            if not song_id:\n",
    "                continue\n",
    "            meta = f\"{song_title} {song_artist}\".lower()\n",
    "            if is_translation_content(meta):\n",
    "                continue\n",
    "            title_sim = compute_similarity(title.lower(), song_title.lower())\n",
    "            artist_sim = compute_similarity((artist or \"\").lower(), song_artist.lower())\n",
    "            combined = (title_sim * 0.7) + (artist_sim * 0.3)\n",
    "            candidates.append({'title': song_title, 'artist': song_artist, 'score': combined})\n",
    "        candidates.sort(key=lambda x: x['score'], reverse=True)\n",
    "        dbg(f\"{len(candidates)} candidates ranked by similarity\")\n",
    "        for cand in candidates:\n",
    "            if cand['score'] < 0.2:\n",
    "                break\n",
    "            try:\n",
    "                song_obj = genius.search_song(cand['title'], cand['artist'])\n",
    "                if not song_obj:\n",
    "                    continue\n",
    "                lyrics_text = getattr(song_obj, \"lyrics\", \"\") or \"\"\n",
    "                if not lyrics_text or len(lyrics_text.strip()) < 30:\n",
    "                    continue\n",
    "                if 'instrumental' in lyrics_text.lower():\n",
    "                    continue\n",
    "                if is_translation_content(lyrics_text):\n",
    "                    continue\n",
    "                info(f\"    Valid lyrics obtained (len={len(lyrics_text)})\")\n",
    "                return lyrics_text\n",
    "            except Exception:\n",
    "                continue\n",
    "        raise RuntimeError(\"No valid lyrics\")\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error: {e}\")\n",
    "\n",
    "def get_genius_lyrics_fallback(artist: str, title: str) -> str:\n",
    "    \"\"\"A simple fallback that queries Genius directly by title and artist.\"\"\"\n",
    "    if not artist or not title:\n",
    "        raise RuntimeError(\"Artist/title missing\")\n",
    "    try:\n",
    "        dbg(f\"    fallback search for '{title}' - '{artist}'\")\n",
    "        song = genius.search_song(title, artist)\n",
    "        if not song:\n",
    "            raise RuntimeError(\"Song not found\")\n",
    "        lyrics_text = getattr(song, \"lyrics\", None)\n",
    "        if not lyrics_text and hasattr(song, 'to_dict'):\n",
    "            sd = song.to_dict()\n",
    "            lyrics_text = sd.get('lyrics', '')\n",
    "        if not lyrics_text or len(lyrics_text.strip()) < 30:\n",
    "            raise RuntimeError(\"Empty or too short lyrics\")\n",
    "        if 'instrumental' in lyrics_text.lower():\n",
    "            raise RuntimeError(\"Instrumental\")\n",
    "        info(f\"    Fallback obtained (len={len(lyrics_text)})\")\n",
    "        return lyrics_text\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fallback error: {e}\")\n",
    "\n",
    "def get_genius_lyrics(artist: str, title: str) -> str:\n",
    "    \"\"\"Try the simple strategy first, then fallback; propagate a single aggregated error on failure.\"\"\"\n",
    "    errors = []\n",
    "    try:\n",
    "        return get_genius_lyrics_simple(artist, title)\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Simple: {e}\")\n",
    "        dbg(f\"simple Genius fetch failed: {e}\")\n",
    "    try:\n",
    "        return get_genius_lyrics_fallback(artist, title)\n",
    "    except Exception as e:\n",
    "        errors.append(f\"Fallback: {e}\")\n",
    "        dbg(f\"fallback Genius fetch failed: {e}\")\n",
    "    raise RuntimeError(\"All Genius methods failed: \" + \" | \".join(errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a406927",
   "metadata": {},
   "source": [
    "Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6536c8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# WHISPER TRANSCRIPTION & POSTPROCESS\n",
    "# -------------------------\n",
    "def transcribe_with_whisper(path: str, model_size: str):\n",
    "    \"\"\"\n",
    "    Transcribe audio at `path` using the Whisper model named `model_size`.\n",
    "    Returns dict with keys:\n",
    "      - 'words': list of (token, start, end)\n",
    "      - 'segments': list of segment metadata {'text','start','end',...}\n",
    "      - 'duration': float (total duration used)\n",
    "    \"\"\"\n",
    "    dbg(f\"loading model '{model_size}'\")\n",
    "    try:\n",
    "        model = whisper.load_model(model_size)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Error loading model '{model_size}': {e}\")\n",
    "\n",
    "    dbg(f\"transcribing with model '{model_size}'\")\n",
    "    try:\n",
    "        # prefer word_timestamps if supported by the API; fallback gracefully\n",
    "        result = model.transcribe(path, word_timestamps=True, language=None, temperature=0.0)\n",
    "    except TypeError:\n",
    "        result = model.transcribe(path, language=None, temperature=0.0)\n",
    "\n",
    "    words_out: List[Tuple[str, float, float]] = []\n",
    "    segments_meta: List[Dict] = []\n",
    "    for seg in result.get(\"segments\", []):\n",
    "        seg_start = float(seg.get(\"start\", 0.0))\n",
    "        seg_end = float(seg.get(\"end\", seg_start + 0.01))\n",
    "        seg_text = seg.get(\"text\", \"\").strip()\n",
    "        seg_meta = {\n",
    "            \"text\": seg_text,\n",
    "            \"start\": seg_start,\n",
    "            \"end\": seg_end,\n",
    "            \"avg_logprob\": seg.get(\"avg_logprob\"),\n",
    "            \"no_speech_prob\": seg.get(\"no_speech_prob\"),\n",
    "        }\n",
    "        segments_meta.append(seg_meta)\n",
    "        if 'words' in seg and isinstance(seg['words'], list) and len(seg['words']) > 0:\n",
    "            for w in seg['words']:\n",
    "                token = w.get('word', '').strip()\n",
    "                if not token:\n",
    "                    continue\n",
    "                s = float(w.get('start', seg_start))\n",
    "                e = float(w.get('end', s + 0.05))\n",
    "                words_out.append((normalize_text(token), s, e))\n",
    "        else:\n",
    "            # fallback: slice segment evenly across words\n",
    "            toks = [t for t in re.split(r\"\\s+\", normalize_text(seg_text)) if t]\n",
    "            if not toks:\n",
    "                continue\n",
    "            seg_len = seg_end - seg_start\n",
    "            per = seg_len / max(1, len(toks))\n",
    "            for i, tok in enumerate(toks):\n",
    "                s = seg_start + i * per\n",
    "                e = min(seg_end, s + per)\n",
    "                words_out.append((tok, s, e))\n",
    "\n",
    "    duration = result.get('duration', words_out[-1][2] if words_out else 0.0)\n",
    "    dbg(f\"raw transcription: {len(words_out)} tokens, duration ~{duration:.2f}s, {len(segments_meta)} segments\")\n",
    "\n",
    "    cleaned_words = clean_transcribed_words(words_out)\n",
    "    dbg(f\"cleaned transcription: {len(cleaned_words)} tokens\")\n",
    "    return {\"words\": cleaned_words, \"segments\": segments_meta, \"duration\": duration}\n",
    "\n",
    "def clean_transcribed_words(words: List[Tuple[str, float, float]]) -> List[Tuple[str, float, float]]:\n",
    "    \"\"\"\n",
    "    Simplified cleaning of word timestamps:\n",
    "      - Merge consecutive identical tokens if the gap is small (<= 0.25s)\n",
    "      - Remove extremely short tokens (duration < 0.03s) and empty tokens\n",
    "    \"\"\"\n",
    "    if not words:\n",
    "        return []\n",
    "\n",
    "    out = []\n",
    "    prev_tok, prev_s, prev_e = words[0]\n",
    "    for tok, s, e in words[1:]:\n",
    "        if tok == prev_tok and (s - prev_e) <= 0.25:\n",
    "            # extend previous token end\n",
    "            prev_e = e\n",
    "        else:\n",
    "            if prev_tok and prev_tok.strip():\n",
    "                out.append((prev_tok, prev_s, prev_e))\n",
    "            prev_tok, prev_s, prev_e = tok, s, e\n",
    "    if prev_tok and prev_tok.strip():\n",
    "        out.append((prev_tok, prev_s, prev_e))\n",
    "\n",
    "    # filter very-short durations (likely noise)\n",
    "    final = []\n",
    "    for tok, s, e in out:\n",
    "        dur = e - s\n",
    "        if dur < 0.03:\n",
    "            continue\n",
    "        final.append((tok, s, e))\n",
    "\n",
    "    return final\n",
    "\n",
    "# -------------------------\n",
    "# TRANSCRIPTION LOGGING HELPERS\n",
    "# -------------------------\n",
    "def save_transcription_files(basename: str, words: List[Tuple[str,float,float]], model_name: str):\n",
    "    \"\"\"Save plain transcripts and per-token timestamps under LOGS_FOLDER for later inspection.\"\"\"\n",
    "    try:\n",
    "        transcript_path = os.path.join(LOGS_FOLDER, f\"{basename}.{model_name}.transcript.txt\")\n",
    "        with open(transcript_path, \"w\", encoding=\"utf-8\") as ft:\n",
    "            ft.write(\" \".join(w[0] for w in words))\n",
    "        ts_path = os.path.join(LOGS_FOLDER, f\"{basename}.{model_name}.whisper.ts.txt\")\n",
    "        with open(ts_path, \"w\", encoding=\"utf-8\") as ft:\n",
    "            for w,s,e in words:\n",
    "                ft.write(f\"{s:.3f}\\t{e:.3f}\\t{w}\\n\")\n",
    "        dbg(f\"saved transcript and timestamps for {basename} ({model_name}) in {LOGS_FOLDER}\")\n",
    "    except Exception as e:\n",
    "        info(f\"warning: error saving transcription files: {e}\")\n",
    "\n",
    "def save_model_logs_text(basename: str, model_name: str, segments_meta, candidate_anchors_all, accepted_candidates, anchors_dict):\n",
    "    \"\"\"\n",
    "    Save a human-readable plain text log to LOGS_FOLDER describing:\n",
    "      - segment text with times\n",
    "      - all anchor candidates with their scores\n",
    "      - accepted anchor indices and final anchor mapping\n",
    "    \"\"\"\n",
    "    try:\n",
    "        path = os.path.join(LOGS_FOLDER, f\"{basename}.{model_name}.log\")\n",
    "        with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"Model: {model_name}\\n\")\n",
    "            f.write(f\"Segments ({len(segments_meta)}):\\n\")\n",
    "            for seg in segments_meta:\n",
    "                f.write(f\"  [{seg.get('start',0):.3f}-{seg.get('end',0):.3f}] {seg.get('text','').strip()}\\n\")\n",
    "            f.write(\"\\nAll candidates (idx, ts, score, overlap):\\n\")\n",
    "            for c in candidate_anchors_all:\n",
    "                f.write(f\"  {c}\\n\")\n",
    "            f.write(\"\\nAccepted candidates after spacing (idx, ts, score, overlap):\\n\")\n",
    "            for a in accepted_candidates:\n",
    "                f.write(f\"  {a}\\n\")\n",
    "            f.write(\"\\nFinal anchors (index -> time):\\n\")\n",
    "            for k,v in sorted(anchors_dict.items()):\n",
    "                f.write(f\"  {k} -> {v:.3f}\\n\")\n",
    "        dbg(f\"saved plain-text logs for {basename} ({model_name}) at {path}\")\n",
    "    except Exception as e:\n",
    "        dbg(f\"error saving logs for {basename} ({model_name}): {e}\")\n",
    "\n",
    "# -------------------------\n",
    "# SILENCE HELPERS & INTERVAL MATH\n",
    "# -------------------------\n",
    "def detect_silences(words: List[Tuple[str,float,float]], min_silence: float = MIN_SILENCE_DURATION) -> List[Tuple[float,float]]:\n",
    "    \"\"\"Return list of (end, next_start) gaps >= min_silence found between consecutive words.\"\"\"\n",
    "    silences = []\n",
    "    if not words:\n",
    "        return silences\n",
    "    for i in range(len(words)-1):\n",
    "        end = words[i][2]\n",
    "        nxt = words[i+1][1]\n",
    "        gap = nxt - end\n",
    "        if gap >= min_silence:\n",
    "            silences.append((end, nxt))\n",
    "    return silences\n",
    "\n",
    "def long_silences_from_words(words: List[Tuple[str,float,float]], threshold: float = LONG_SILENCE_THRESHOLD) -> List[Tuple[float,float]]:\n",
    "    \"\"\"Return silences considered 'long' (>= threshold).\"\"\"\n",
    "    return [sil for sil in detect_silences(words, min_silence=0.0) if (sil[1] - sil[0]) >= threshold]\n",
    "\n",
    "def subtract_long_silences_from_interval(a: float, b: float, long_silences: List[Tuple[float,float]]) -> List[Tuple[float,float]]:\n",
    "    \"\"\"\n",
    "    Subtract long silent intervals from [a,b] and return remaining active intervals.\n",
    "    Useful for distributing interpolated timestamps only across 'active' audio.\n",
    "    \"\"\"\n",
    "    if a >= b:\n",
    "        return []\n",
    "    intervals = [(a,b)]\n",
    "    for s,e in sorted(long_silences):\n",
    "        new_intervals = []\n",
    "        for (x,y) in intervals:\n",
    "            if e <= x or s >= y:\n",
    "                new_intervals.append((x,y))\n",
    "            else:\n",
    "                if s > x:\n",
    "                    new_intervals.append((x, min(s,y)))\n",
    "                if e < y:\n",
    "                    new_intervals.append((max(e,x), y))\n",
    "        intervals = new_intervals\n",
    "        if not intervals:\n",
    "            break\n",
    "    intervals = [(max(a,x), min(b,y)) for (x,y) in intervals if max(a,x) < min(b,y)]\n",
    "    intervals.sort()\n",
    "    return intervals\n",
    "\n",
    "def distribute_across_active_intervals(n_items: int, active_intervals: List[Tuple[float,float]]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Equally distribute n_items across a list of active intervals proportionally to each interval's length.\n",
    "    Returns a list of timestamps (one per item) spaced throughout active audio.\n",
    "    \"\"\"\n",
    "    if n_items <= 0:\n",
    "        return []\n",
    "    durations = [y - x for (x,y) in active_intervals]\n",
    "    total = sum(durations)\n",
    "    if total <= 1e-6:\n",
    "        return [0.0] * n_items\n",
    "    timestamps = []\n",
    "    for j in range(1, n_items + 1):\n",
    "        frac = j / (n_items + 1)\n",
    "        offset = frac * total\n",
    "        acc = 0.0\n",
    "        for (iv_start, iv_end), iv_len in zip(active_intervals, durations):\n",
    "            if acc + iv_len >= offset - 1e-9:\n",
    "                within = offset - acc\n",
    "                ts = iv_start + within\n",
    "                timestamps.append(ts)\n",
    "                break\n",
    "            acc += iv_len\n",
    "    if len(timestamps) < n_items and active_intervals:\n",
    "        iv_start, iv_end = active_intervals[0]\n",
    "        for _ in range(n_items - len(timestamps)):\n",
    "            timestamps.append(iv_start + 0.1 * (_+1))\n",
    "    return timestamps\n",
    "\n",
    "# -------------------------\n",
    "# MATCHING HELPERS\n",
    "# -------------------------\n",
    "def compute_similarity(a: str, b: str) -> float:\n",
    "    \"\"\"Return a normalized similarity (0..1) between two strings using SequenceMatcher.\"\"\"\n",
    "    a_tok = normalize_text(a)\n",
    "    b_tok = normalize_text(b)\n",
    "    if not a_tok or not b_tok:\n",
    "        return 0.0\n",
    "    return difflib.SequenceMatcher(None, a_tok, b_tok).ratio()\n",
    "\n",
    "def compute_overlap_fraction(line_words: List[str], segment_words: List[str]) -> float:\n",
    "    \"\"\"Return fraction of words in `line_words` that appear in `segment_words` (exact normalized match).\"\"\"\n",
    "    if not line_words:\n",
    "        return 0.0\n",
    "    line_set = [normalize_text(w) for w in line_words]\n",
    "    seg_set = [normalize_text(w) for w in segment_words]\n",
    "    matches = 0\n",
    "    for w in line_set:\n",
    "        if w and w in seg_set:\n",
    "            matches += 1\n",
    "    return matches / len(line_set)\n",
    "\n",
    "def find_best_match_for_line_strict(line: str, words_timing: List[Tuple[str,float,float]], segments_text: Optional[List[Tuple[str,float,float]]] = None) -> Tuple[Optional[int], float, float, int]:\n",
    "    \"\"\"\n",
    "    For a lyric line, attempt to find the best matching window of tokens in words_timing.\n",
    "    Returns (start_index_or_None, best_score, best_timestamp, matched_length).\n",
    "    If start_index is None, best_timestamp is derived from segment-level match.\n",
    "    \"\"\"\n",
    "    if not line.strip() or not words_timing:\n",
    "        return None, 0.0, 0.0, 0\n",
    "    lyric_words = [w for w in re.split(r\"\\s+\", normalize_text(line)) if w]\n",
    "    if not lyric_words:\n",
    "        return None, 0.0, 0.0, 0\n",
    "    trans_words = [w[0] for w in words_timing]\n",
    "    n = len(trans_words)\n",
    "    base_len = len(lyric_words)\n",
    "    best_pos = None\n",
    "    best_score = 0.0\n",
    "    best_ts = 0.0\n",
    "    best_len = 0\n",
    "    # try windows around expected lyric word length\n",
    "    for L in range(max(1, base_len - 1), base_len + 2):\n",
    "        for i in range(0, n - L + 1):\n",
    "            seg = trans_words[i:i+L]\n",
    "            seg_text = \" \".join(seg)\n",
    "            sim = compute_similarity(line, seg_text)\n",
    "            overlap = compute_overlap_fraction(lyric_words, seg)\n",
    "            score = sim * 0.85 + (0.15 * overlap)\n",
    "            len_penalty = 1.0 - abs(len(seg) - base_len) / max(1, base_len)\n",
    "            score *= (0.9 + 0.1 * len_penalty)\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_pos = i\n",
    "                best_ts = words_timing[i][1]\n",
    "                best_len = L\n",
    "    # fallback to segment-level matches if any segment text closely matches the line\n",
    "    if segments_text:\n",
    "        for seg_text, seg_start, seg_end in segments_text:\n",
    "            seg_sim = compute_similarity(line, seg_text)\n",
    "            if seg_sim > 0.55:\n",
    "                seg_score = seg_sim + 0.10\n",
    "                if seg_score > best_score:\n",
    "                    best_score = seg_score\n",
    "                    best_pos = None\n",
    "                    best_ts = seg_start\n",
    "                    best_len = max(1, len([w for w in re.split(r\"\\s+\", seg_text) if w]))\n",
    "    return best_pos, best_score, best_ts, best_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378fea55",
   "metadata": {},
   "source": [
    "Raw lyrics filtering and LRC building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aeb032a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# DUPLICATE DETECTION (UNION-FIND)\n",
    "# -------------------------\n",
    "def normalize_for_dup_check(s: str) -> str:\n",
    "    \"\"\"Normalized text for duplicate detection (strip parentheses/brackets and punctuation).\"\"\"\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    t = s\n",
    "    t = re.sub(r'\\([^)]*\\)', ' ', t)\n",
    "    t = re.sub(r'\\[[^\\]]*\\]', ' ', t)\n",
    "    t = re.sub(r\"[^\\w\\s']\", ' ', t)\n",
    "    t = re.sub(r'\\s+', ' ', t).strip().lower()\n",
    "    return t\n",
    "\n",
    "def jaccard_tokens(a: str, b: str) -> float:\n",
    "    \"\"\"Compute Jaccard between token sets of two normalized strings.\"\"\"\n",
    "    aset = set([w for w in re.split(r'\\s+', normalize_for_dup_check(a)) if w])\n",
    "    bset = set([w for w in re.split(r'\\s+', normalize_for_dup_check(b)) if w])\n",
    "    if not aset and not bset:\n",
    "        return 0.0\n",
    "    inter = aset.intersection(bset)\n",
    "    union = aset.union(bset)\n",
    "    return len(inter) / len(union) if union else 0.0\n",
    "\n",
    "class UnionFind:\n",
    "    \"\"\"Small union-find helper used for clustering repeated lyric lines.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.parent = list(range(n))\n",
    "    def find(self, x):\n",
    "        p = self.parent\n",
    "        while p[x] != x:\n",
    "            p[x] = p[p[x]]\n",
    "            x = p[x]\n",
    "        return x\n",
    "    def union(self, a, b):\n",
    "        ra = self.find(a); rb = self.find(b)\n",
    "        if ra == rb:\n",
    "            return\n",
    "        self.parent[rb] = ra\n",
    "\n",
    "def cluster_similar_lines_robust(lines: List[str]) -> Tuple[List[int], Dict[int, List[int]]]:\n",
    "    \"\"\"\n",
    "    Cluster similar lyric lines to avoid using repeated lines (chorus repeats) as anchors.\n",
    "    Hardcoded thresholds (for publication simplicity):\n",
    "      - Sequence similarity threshold: 0.92\n",
    "      - Jaccard tokens threshold: 0.75\n",
    "    Returns:\n",
    "      - cluster_id_by_index: list mapping line index -> cluster id\n",
    "      - clusters: dict cluster_id -> list of indices in that cluster\n",
    "    \"\"\"\n",
    "    seq_thresh = 0.92\n",
    "    jaccard_thresh = 0.75\n",
    "    n = len(lines)\n",
    "    if n == 0:\n",
    "        return [], {}\n",
    "    uf = UnionFind(n)\n",
    "    normalized = [normalize_for_dup_check(l) for l in lines]\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            a = normalized[i]; b = normalized[j]\n",
    "            if not a or not b:\n",
    "                continue\n",
    "            seq_sim = difflib.SequenceMatcher(None, a, b).ratio()\n",
    "            jac = jaccard_tokens(a, b)\n",
    "            if seq_sim >= seq_thresh or jac >= jaccard_thresh:\n",
    "                uf.union(i, j)\n",
    "    root_to_cid = {}\n",
    "    clusters = {}\n",
    "    cluster_id_by_index = [-1] * n\n",
    "    next_cid = 0\n",
    "    for i in range(n):\n",
    "        r = uf.find(i)\n",
    "        if r not in root_to_cid:\n",
    "            root_to_cid[r] = next_cid\n",
    "            clusters[next_cid] = []\n",
    "            next_cid += 1\n",
    "        cid = root_to_cid[r]\n",
    "        clusters[cid].append(i)\n",
    "        cluster_id_by_index[i] = cid\n",
    "    dbg(f\"clustered {n} lines into {len(clusters)} clusters\")\n",
    "    return cluster_id_by_index, clusters\n",
    "\n",
    "# -------------------------\n",
    "# WRITE LRC HELPERS\n",
    "# -------------------------\n",
    "def write_lrc(basename: str, lines: List[str], times: List[float]):\n",
    "    \"\"\"Write a standard LRC file into LYRICS_FOLDER.\"\"\"\n",
    "    out_path = os.path.join(LYRICS_FOLDER, f\"{basename}.lrc\")\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for line, t in zip(lines, times):\n",
    "            m = int(t // 60)\n",
    "            s = t % 60\n",
    "            f.write(f\"[{m:02d}:{s:05.2f}]{line}\\n\")\n",
    "    dbg(f\"written LRC to {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "def enforce_monotonic(times: List[float], min_prog: float = MIN_LINE_PROGRESSION) -> List[float]:\n",
    "    \"\"\"Ensure times are strictly increasing; adjust with a minimal progression if needed.\"\"\"\n",
    "    out = []\n",
    "    last = -1e9\n",
    "    for t in times:\n",
    "        if t <= last + 1e-9:\n",
    "            t = last + min_prog\n",
    "        out.append(t)\n",
    "        last = t\n",
    "    return out\n",
    "\n",
    "# -------------------------\n",
    "# ANCHORS COMPUTATION\n",
    "# -------------------------\n",
    "def compute_anchors_from_transcription(lines: List[str], words: List[Tuple[str,float,float]], segments_text: List[Tuple[str,float,float]], long_silences: List[Tuple[float,float]], cluster_ids: List[int],\n",
    "                                       repeated_cluster_ids: set) -> Tuple[Dict[int,float], float, List[Tuple[int,float,float,float]], List[Tuple[int,float,float,float]]]:\n",
    "    \"\"\"\n",
    "    Given lyric lines and transcription tokens, return:\n",
    "      - anchors: dict index -> timestamp\n",
    "      - score_sum: sum of accepted anchor scores\n",
    "      - accepted_candidates: list accepted candidate tuples\n",
    "      - all_candidates: list of all candidate tuples\n",
    "    Notes:\n",
    "      - We no longer require a minimum number of anchor words; attempts are made for all lines,\n",
    "        but repeated-line clusters are excluded from candidates to avoid false duplicates.\n",
    "    \"\"\"\n",
    "    candidate_anchors_all = []\n",
    "    for idx, line in enumerate(lines):\n",
    "        if cluster_ids and cluster_ids[idx] in repeated_cluster_ids:\n",
    "            # skip this line since it belongs to a repeated cluster\n",
    "            continue\n",
    "        pos, score, ts, seg_len = find_best_match_for_line_strict(line, words, segments_text)\n",
    "        if pos is not None or score > 0:\n",
    "            lyric_words = [w for w in re.split(r\"\\s+\", normalize_text(line)) if w]\n",
    "            if pos is not None and seg_len > 0:\n",
    "                seg_words = [w[0] for w in words[pos:pos+seg_len]]\n",
    "            else:\n",
    "                seg_words = [w[0] for w in words if abs(w[1] - ts) < 2.0][:len(lyric_words)]\n",
    "            overlap = compute_overlap_fraction(lyric_words, seg_words)\n",
    "            candidate_anchors_all.append((idx, ts, score, overlap))\n",
    "    candidate_anchors_all.sort(key=lambda x: x[1])\n",
    "    anchors: Dict[int, float] = {}\n",
    "    last_anchor_time = -9999.0\n",
    "    score_sum = 0.0\n",
    "    accepted_candidates = []\n",
    "    for idx, ts, score, overlap in candidate_anchors_all:\n",
    "        if score >= THRESH_ANCHOR and overlap >= MIN_OVERLAP and ts > last_anchor_time + MIN_ANCHOR_SPACING:\n",
    "            inside_long = any(s <= ts <= e for (s,e) in long_silences)\n",
    "            if inside_long:\n",
    "                dbg(f\"candidate idx={idx} ts={ts:.2f} skipped because inside long silence\")\n",
    "                continue\n",
    "            anchors[idx] = ts\n",
    "            last_anchor_time = ts\n",
    "            score_sum += score\n",
    "            accepted_candidates.append((idx, ts, score, overlap))\n",
    "    dbg(f\"accepted anchors: {len(anchors)}, score_sum: {score_sum:.3f}\")\n",
    "    return anchors, score_sum, accepted_candidates, candidate_anchors_all\n",
    "\n",
    "def compute_final_times_from_anchors(lines: List[str], anchors: Dict[int,float], words: List[Tuple[str,float,float]], long_silences: List[Tuple[float,float]], duration: float) -> List[float]:\n",
    "    \"\"\"\n",
    "    Given anchors mapping, interpolate times for all lines:\n",
    "      - For lines before first anchor: use active audio intervals or weighted equispacing with FIRST_LINE_WEIGHT\n",
    "      - Between anchors: distribute across active intervals or fallback spacing\n",
    "      - After last anchor: distribute in tail active intervals or use fallback spacing\n",
    "    Returns list of timestamps (len == len(lines)).\n",
    "    \"\"\"\n",
    "    final_times: List[Optional[float]] = [None] * len(lines)\n",
    "    for idx, t in anchors.items():\n",
    "        final_times[idx] = t\n",
    "\n",
    "    anchor_indices = sorted(anchors.keys())\n",
    "    if not anchor_indices:\n",
    "        # no reliable anchors at all: uniform distribution across duration\n",
    "        n = len(lines)\n",
    "        return enforce_monotonic([i * (duration / max(1, n-1)) for i in range(n)])\n",
    "\n",
    "    # HEAD region (before first anchor)\n",
    "    first_idx = anchor_indices[0]\n",
    "    first_t = anchors[first_idx]\n",
    "    if first_idx > 0:\n",
    "        active_head = subtract_long_silences_from_interval(0.0, first_t, long_silences)\n",
    "        if active_head:\n",
    "            times_head = distribute_across_active_intervals(first_idx, active_head)\n",
    "            for i, tt in enumerate(times_head):\n",
    "                final_times[i] = tt\n",
    "        else:\n",
    "            # weighted equispacing: first line receives FIRST_LINE_WEIGHT portion\n",
    "            n = first_idx\n",
    "            weights = [FIRST_LINE_WEIGHT] + [1] * (n - 1)\n",
    "            total_weight = sum(weights)\n",
    "            cum = 0.0\n",
    "            for i, w in enumerate(weights):\n",
    "                cum += w\n",
    "                final_times[i] = (cum / total_weight) * first_t\n",
    "\n",
    "    # BETWEEN anchors\n",
    "    for a_i, b_i in zip(anchor_indices, anchor_indices[1:]):\n",
    "        ta = anchors[a_i]; tb = anchors[b_i]\n",
    "        segment = list(range(a_i+1, b_i))\n",
    "        if not segment:\n",
    "            continue\n",
    "        active_intervals = subtract_long_silences_from_interval(ta, tb, long_silences)\n",
    "        if active_intervals and sum((y-x) for (x,y) in active_intervals) > 1e-6:\n",
    "            timestamps = distribute_across_active_intervals(len(segment), active_intervals)\n",
    "            for li, ts in zip(segment, timestamps):\n",
    "                final_times[li] = ts\n",
    "        else:\n",
    "            spacing = 1.5\n",
    "            cand_times = []\n",
    "            for j in range(1, len(segment) + 1):\n",
    "                cand = ta + j * spacing\n",
    "                shifted = cand\n",
    "                for s,e in long_silences:\n",
    "                    if s < cand < e:\n",
    "                        shifted = e + 0.05\n",
    "                        break\n",
    "                if shifted >= tb:\n",
    "                    shifted = ta + (tb - ta) * (j / (len(segment) + 1))\n",
    "                cand_times.append(min(tb - 0.01, shifted))\n",
    "            for li, ts in zip(segment, cand_times):\n",
    "                final_times[li] = ts\n",
    "\n",
    "    # TAIL region (after last anchor)\n",
    "    last_idx = anchor_indices[-1]\n",
    "    last_t = anchors[last_idx]\n",
    "    if last_idx < len(lines) - 1:\n",
    "        active_tail = subtract_long_silences_from_interval(last_t, duration, long_silences)\n",
    "        n_tail = len(lines) - 1 - last_idx\n",
    "        if active_tail and sum((y-x) for (x,y) in active_tail) > 1e-6:\n",
    "            times_tail = distribute_across_active_intervals(n_tail, active_tail)\n",
    "            for k, tt in enumerate(times_tail, start=1):\n",
    "                final_times[last_idx + k] = tt\n",
    "        else:\n",
    "            for k in range(1, len(lines) - last_idx):\n",
    "                final_times[last_idx + k] = min(duration, final_times[last_idx + k - 1] + FALLBACK_SPACING)\n",
    "\n",
    "    # Any remaining None -> fallback uniform distribution\n",
    "    for i in range(len(final_times)):\n",
    "        if final_times[i] is None:\n",
    "            final_times[i] = min(duration, i * (duration / max(1, len(lines) - 1)))\n",
    "\n",
    "    final_times = enforce_monotonic(final_times)\n",
    "    return final_times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d2800",
   "metadata": {},
   "source": [
    "Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfe2c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# PER-FILE HIGH-LEVEL FLOW\n",
    "# -------------------------\n",
    "def process_file_with_model_comparison(flac_path: str, genius_client: Optional[Genius], artist_meta: Optional[str]=None, title_meta: Optional[str]=None, db_already_checked: bool=False) -> bool:\n",
    "    \"\"\"\n",
    "    Main per-file processing:\n",
    "      - attempt to restore an exact match from .lyrics_db (skip processing if found)\n",
    "      - obtain lyrics (local .txt or Genius)\n",
    "      - perform transcription with each model in TRANSCRIBE_MODELS\n",
    "      - compute anchors for each model, keep logs\n",
    "      - select the best model based on anchors_count then score_sum\n",
    "      - write final .lrc and copy to DB (title/artist meta required)\n",
    "    Returns True if at least one anchor was produced by the chosen model; False otherwise.\n",
    "    \"\"\"\n",
    "    basename = os.path.splitext(os.path.basename(flac_path))[0]\n",
    "    txt_path = os.path.join(SONGS_FOLDER, f\"{basename}.txt\")\n",
    "    lyrics = None\n",
    "\n",
    "    # Only extract metadata if not provided by caller (prevents duplicated debug messages).\n",
    "    if artist_meta is None or title_meta is None:\n",
    "        try:\n",
    "            artist_meta, title_meta = extract_metadata_from_flac(flac_path)\n",
    "        except Exception:\n",
    "            artist_meta, title_meta = None, None\n",
    "    else:\n",
    "        dbg(f\"metadata provided by caller: artist='{artist_meta}', title='{title_meta}'\")\n",
    "\n",
    "    lrc_target_path = os.path.join(LYRICS_FOLDER, f\"{basename}.lrc\")\n",
    "    # Early restore: only attempt if caller did NOT already check the DB.\n",
    "    if (not db_already_checked) and title_meta and artist_meta:\n",
    "        try:\n",
    "            if restore_from_lyrics_db(title_meta, artist_meta, lrc_target_path):\n",
    "                info(f\"Restored exact LRC from DB: '{title_meta}' - '{artist_meta}' (skipping transcription)\")\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            dbg(f\"error during early restore: {e}\")\n",
    "\n",
    "    # If a local .txt with lyrics exists, use it\n",
    "    if os.path.exists(txt_path):\n",
    "        try:\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lyrics = f.read()\n",
    "            info(f\"Using local lyrics file {txt_path}\")\n",
    "        except Exception as e:\n",
    "            info(f\"warning: could not read {txt_path}: {e}\")\n",
    "\n",
    "    # fallback: try to find a similar entry in the DB\n",
    "    if not lyrics and title_meta and artist_meta:\n",
    "        similar = search_similar_in_db(title_meta, artist_meta, similarity_threshold=0.82)\n",
    "        if similar:\n",
    "            try:\n",
    "                shutil.copy2(similar, lrc_target_path)\n",
    "                info(f\"Restored similar LRC from DB: {os.path.basename(similar)}\")\n",
    "                copy_to_lyrics_db(lrc_target_path, title_meta, artist_meta)\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                info(f\"warning: error copying similar DB file: {e}\")\n",
    "\n",
    "    # If still not found, attempt Genius (if available)\n",
    "    if not lyrics:\n",
    "        if not title_meta:\n",
    "            info(f\"warning: missing title metadata for {basename}; cannot fetch lyrics\")\n",
    "            return False\n",
    "        if not genius_client:\n",
    "            info(f\"warning: no Genius client and no local '{basename}.txt' found; skipping\")\n",
    "            return False\n",
    "        try:\n",
    "            lyrics = get_genius_lyrics(artist_meta, title_meta)\n",
    "            if not lyrics:\n",
    "                info(\"warning: Genius returned empty lyrics\")\n",
    "                return False\n",
    "            # Save raw lyrics to logs ONLY (user requested raw lyrics only in logs)\n",
    "            try:\n",
    "                raw_path = os.path.join(LOGS_FOLDER, f\"{basename}.raw_lyrics.txt\")\n",
    "                with open(raw_path, \"w\", encoding=\"utf-8\") as rf:\n",
    "                    rf.write(lyrics)\n",
    "                dbg(f\"saved raw lyrics into {raw_path}\")\n",
    "            except Exception as e:\n",
    "                dbg(f\"could not save raw lyrics: {e}\")\n",
    "            # Do NOT write lyrics into songs/ (user requested that raw lyrics should not be created there)\n",
    "        except Exception as e:\n",
    "            info(f\"error obtaining lyrics from Genius: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Split into cleaned lyric lines\n",
    "    lines = split_lyrics_lines(lyrics)\n",
    "    if not lines:\n",
    "        info(f\"warning: lyrics empty after splitting for {basename}\")\n",
    "        return False\n",
    "\n",
    "    # Cluster similar lines to avoid repeated-line anchor usage (choruses, repeats)\n",
    "    cluster_ids, clusters = cluster_similar_lines_robust(lines)\n",
    "    repeated_cluster_ids = {cid for cid, members in clusters.items() if len(members) > 1}\n",
    "    dbg(f\"{len(clusters)} clusters detected, {len(repeated_cluster_ids)} are repeated clusters\")\n",
    "\n",
    "    # Transcribe with all configured models, save logs and compute anchors\n",
    "    model_results = []\n",
    "    for model_name in TRANSCRIBE_MODELS:\n",
    "        try:\n",
    "            dbg(f\"transcribing {basename} using model '{model_name}'\")\n",
    "            transcribed = transcribe_with_whisper(flac_path, model_name)\n",
    "            words = transcribed[\"words\"]\n",
    "            segments_meta = transcribed.get(\"segments\", [])\n",
    "            duration = transcribed.get(\"duration\", words[-1][2] if words else 180.0)\n",
    "            dbg(f\"model '{model_name}' produced {len(words)} tokens, duration ~{duration:.2f}s\")\n",
    "            if not words:\n",
    "                dbg(f\"model '{model_name}' returned no words\")\n",
    "                model_results.append((model_name, None))\n",
    "                continue\n",
    "\n",
    "            save_transcription_files(basename, words, model_name)\n",
    "\n",
    "            # Build segments_text using a fixed gap threshold (1.0s) for simplicity here\n",
    "            segments_text = []\n",
    "            cur_words = [words[0][0]]\n",
    "            cur_start = words[0][1]\n",
    "            cur_end = words[0][2]\n",
    "            for tok, s, e in words[1:]:\n",
    "                gap = s - cur_end\n",
    "                if gap > 1.0:\n",
    "                    segments_text.append((\" \".join(cur_words), cur_start, cur_end))\n",
    "                    cur_words = [tok]\n",
    "                    cur_start = s\n",
    "                    cur_end = e\n",
    "                else:\n",
    "                    cur_words.append(tok)\n",
    "                    cur_end = e\n",
    "            segments_text.append((\" \".join(cur_words), cur_start, cur_end))\n",
    "\n",
    "            long_silences = long_silences_from_words(words, threshold=LONG_SILENCE_THRESHOLD)\n",
    "\n",
    "            anchors, score_sum, accepted_candidates, all_candidates = compute_anchors_from_transcription(\n",
    "                lines, words, segments_text, long_silences, cluster_ids, repeated_cluster_ids\n",
    "            )\n",
    "            anchors_count = len(anchors)\n",
    "            dbg(f\"model '{model_name}' => anchors_count={anchors_count}, score_sum={score_sum:.3f}\")\n",
    "\n",
    "            save_model_logs_text(basename, model_name, segments_meta, all_candidates, accepted_candidates, anchors)\n",
    "\n",
    "            final_times = compute_final_times_from_anchors(lines, anchors, words, long_silences, duration)\n",
    "            model_results.append((model_name, {\n",
    "                \"anchors\": anchors,\n",
    "                \"anchors_count\": anchors_count,\n",
    "                \"score_sum\": score_sum,\n",
    "                \"final_times\": final_times,\n",
    "                \"words\": words,\n",
    "                \"segments_text\": segments_text,\n",
    "                \"duration\": duration,\n",
    "            }))\n",
    "        except Exception as e:\n",
    "            dbg(f\"error with model '{model_name}': {e}\")\n",
    "            model_results.append((model_name, None))\n",
    "\n",
    "    # Choose the best model by anchors_count first, then by score_sum\n",
    "    best_candidate = None\n",
    "    best_metrics = (-1, -1.0)\n",
    "    for model_name, data in model_results:\n",
    "        if not data:\n",
    "            continue\n",
    "        ac = data[\"anchors_count\"]\n",
    "        ss = data[\"score_sum\"]\n",
    "        if ac > best_metrics[0] or (ac == best_metrics[0] and ss > best_metrics[1]):\n",
    "            best_metrics = (ac, ss)\n",
    "            best_candidate = (model_name, data)\n",
    "\n",
    "    if not best_candidate:\n",
    "        info(f\"No model produced valid anchors for '{basename}'. Writing fallback equispaced LRC.\")\n",
    "        # choose fallback duration from any model if available\n",
    "        fallback_duration = 180.0\n",
    "        for _, data in model_results:\n",
    "            if data and data.get(\"duration\"):\n",
    "                fallback_duration = data[\"duration\"]\n",
    "                break\n",
    "        times = [i * (fallback_duration / max(1, len(lines) - 1)) for i in range(len(lines))]\n",
    "        times = enforce_monotonic(times)\n",
    "        lrc_path = write_lrc(basename, lines, times)\n",
    "        if title_meta and artist_meta:\n",
    "            copy_to_lyrics_db(lrc_path, title_meta, artist_meta)\n",
    "        info(f\"Fallback LRC written: {lrc_path}\")\n",
    "        return False\n",
    "\n",
    "    chosen_model, chosen_data = best_candidate\n",
    "    info(f\"Chosen model for '{basename}': {chosen_model} (anchors={chosen_data['anchors_count']}, score_sum={chosen_data['score_sum']:.3f})\")\n",
    "\n",
    "    final_times = chosen_data[\"final_times\"]\n",
    "    lrc_path = write_lrc(basename, lines, final_times)\n",
    "    if title_meta and artist_meta:\n",
    "        copy_to_lyrics_db(lrc_path, title_meta, artist_meta)\n",
    "    info(f\"LRC generated: {lrc_path}\")\n",
    "    return chosen_data[\"anchors_count\"] > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cfc39e",
   "metadata": {},
   "source": [
    "Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f8079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------\n",
    "# MAIN\n",
    "# -------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    info(\"Anchors-Strict LRC Generator starting\")\n",
    "    initialize_folders()\n",
    "    ingest_existing_lrcs_and_cleanup_songs()\n",
    "\n",
    "    creds = load_credentials(CREDENTIALS_PATH)\n",
    "    GENIUS_ACCESS_TOKEN = creds.get(\"genius_access_token\")\n",
    "    genius = None\n",
    "    if GENIUS_ACCESS_TOKEN:\n",
    "        genius = get_genius_client(GENIUS_ACCESS_TOKEN)\n",
    "        if genius:\n",
    "            info(\"Genius client configured\")\n",
    "            try:\n",
    "                # quick connectivity sanity check\n",
    "                _ = genius.search_songs(\"test\", per_page=1)\n",
    "                dbg(\"genius connectivity OK\")\n",
    "            except Exception as e:\n",
    "                dbg(f\"genius test issue: {e}\")\n",
    "        else:\n",
    "            info(\"warning: Genius token provided but initialization failed\")\n",
    "    else:\n",
    "        info(\"note: no Genius token in credentials.json; Genius lookups will be skipped\")\n",
    "\n",
    "    flac_files = find_flac_files(SONGS_FOLDER)\n",
    "    if not flac_files:\n",
    "        info(f\"No FLAC files found in '{SONGS_FOLDER}'\")\n",
    "    else:\n",
    "        info(f\"Found {len(flac_files)} files in {SONGS_FOLDER}\")\n",
    "        processed = 0\n",
    "        failed = 0\n",
    "        no_anchor_files = []\n",
    "        for i, flac in enumerate(flac_files, 1):\n",
    "            info(f\"[{i}/{len(flac_files)}] Processing: {os.path.basename(flac)}\")\n",
    "\n",
    "            # Early check: if DB already contains a matching Title-Artist entry restore and SKIP\n",
    "            try:\n",
    "                artist_meta, title_meta = extract_metadata_from_flac(flac)\n",
    "            except Exception:\n",
    "                artist_meta, title_meta = None, None\n",
    "            if title_meta and artist_meta:\n",
    "                lrc_target_path = os.path.join(LYRICS_FOLDER, f\"{os.path.splitext(os.path.basename(flac))[0]}.lrc\")\n",
    "                try:\n",
    "                    if restore_from_lyrics_db(title_meta, artist_meta, lrc_target_path):\n",
    "                        info(f\"Restored from DB (skipping transcription): '{title_meta}' - '{artist_meta}'\")\n",
    "                        processed += 1\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    dbg(f\"error during early DB restore: {e}\")\n",
    "\n",
    "            try:\n",
    "                ok_has_anchors = process_file_with_model_comparison(flac, genius, artist_meta=artist_meta, title_meta=title_meta, db_already_checked=True)\n",
    "                if ok_has_anchors:\n",
    "                    processed += 1\n",
    "                else:\n",
    "                    no_anchor_files.append(os.path.basename(flac))\n",
    "                    failed += 1\n",
    "            except Exception as e:\n",
    "                info(f\"error processing {os.path.basename(flac)}: {e}\")\n",
    "                failed += 1\n",
    "\n",
    "        info(\"\\nSummary:\")\n",
    "        info(f\"  processed with anchors: {processed}\")\n",
    "        info(f\"  without anchors / problematic: {failed}\")\n",
    "\n",
    "        # Write the list of files where no reliable anchors were found\n",
    "        try:\n",
    "            with open(NO_ANCHORS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "                for name in no_anchor_files:\n",
    "                    f.write(name + \"\\n\")\n",
    "            info(f\"  No-anchors file written: {NO_ANCHORS_FILE} (count={len(no_anchor_files)})\")\n",
    "        except Exception as e:\n",
    "            info(f\"warning: could not write {NO_ANCHORS_FILE}: {e}\")\n",
    "\n",
    "        db_files = glob.glob(os.path.join(LYRICS_DB_FOLDER, \"*.lrc\"))\n",
    "        dbg(f\"DB files count: {len(db_files)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "video",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
